---
title: "Predicting Reading Score"
author: "Jeong Yun Choi"
date: "2024-12-15"
output: github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
library(PerformanceAnalytics)
library(rstatix)
library(gtools)
library(tidyverse)
library(ggplot2)
library(caret)
library(ggpubr)
library(dplyr)

knitr::opts_chunk$set(
	include = TRUE,
	warning = FALSE,
	fig.width = 7, 
  fig.height = 5,
  out.width = "90%", 
	fig.align = "center"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal())
```

```{r, include=FALSE}
score_df <- read_csv("Project_1_data.csv") |>  
  janitor::clean_names() |>  
  mutate(wkly_study_hours = 
           case_match(
             wkly_study_hours, 
             "< 5" ~ "< 5",
             "> 10" ~ "> 10", 
             "10-May" ~ "5-10"), 
         wkly_study_hours = factor(wkly_study_hours, c("< 5", "5-10", "> 10")), 
         lunch_type = fct_relevel(lunch_type, "standard"), 
         parent_marital_status = fct_relevel(parent_marital_status, "single"), 
         transport_means = fct_relevel(transport_means, "school_bus"), 
         parent_educ = fct_relevel(parent_educ, "some high school")) %>% 
  drop_na() %>% 
  select(,-c("math_score","writing_score"))
```

We implement step-wise regression to deduce significant covariates for modeling `reading score`. We note that most of the variables listed are categorical as mentioned prior. Therefore, we incorporated factor releveing in order to give some dimensions. First, we will check for non-linearity in the `reading_score` and apply transformation if need be. 

```{r Preliminary Model 1 for Reading Score}

# Preliminary using all covariates for the first model
reading_model1v1 <- lm(reading_score ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + practice_sport + transport_means + wkly_study_hours, data = score_df)

summary(reading_model1v1)


###covariates practice_sports, is_first_child, nr_siblings, transport_means are not significant.
reading_model1v2 <- lm(reading_score ~ gender + ethnic_group + parent_educ +lunch_type + test_prep + parent_marital_status + wkly_study_hours, data = score_df)
summary(reading_model1v2)

#increased adjusted R^2 value. Will be keeping this model.

reading_model1<- lm(reading_score ~ gender + ethnic_group + parent_educ +lunch_type + test_prep + parent_marital_status + wkly_study_hours, data = score_df)

par(mfrow = c(1,1))
boxcox_reading <- MASS::boxcox(reading_model1, lambda = seq(-2.5, 2.5, 0.1))
shapiro.test(rstandard(reading_model1))

boxcox_reading <- Reduce(cbind, boxcox_reading)
optimal_power <- boxcox_reading |> 
  as_tibble() |> 
  filter(V2 == max(V2)) |> 
  pull(init) |> 
  round(digits =2)

```

```{r Transformation }
# Transformed Y using the optimal lambda: 
score_df <- score_df %>% 
  mutate(transformed_reading = (reading_score +1)^1.44)

#adjusted some relevant covariates
reading_model1 = lm(transformed_reading ~ gender + ethnic_group + parent_educ + test_prep + parent_marital_status + wkly_study_hours, data = score_df)

reading_model1 %>% 
  broom::tidy() %>% 
  mutate(`p.value` = signif(`p.value`, 3), 
         estimate = round(estimate, 3), 
         `std.error` = round(`std.error`, 3), 
         statistic = round(statistic, 3)) |> 
  knitr::kable(digits = 50)


par(mfrow = c(2,2))
plot(reading_model1)
shapiro.test(rstandard(reading_model1))
##After transforming, shapiro test fails to reject null, indicating normality in the distribution 

### removing the influential point if needed
cooksd_read <- cooks.distance(reading_model1)
influential_read <- as.numeric(names(cooksd_read)[(cooksd_read > 0.5)])
# influential_read is 0

```
As seen above the after Y transformation where $Y^*= ({reading\_score + 1})^{1.44}$, the residuals follow normality, homoscedascity and mean zero looking at the diagnostic plots. 

## Refined Covariates
```{r}
### Add interaction between `ethnic_group` and `parent_educ`.
reading_model2v1 <- lm(transformed_reading ~ gender + test_prep + parent_marital_status + wkly_study_hours + ethnic_group * parent_educ, data = score_df)
summary(reading_model2v1)

### No significant interaction effect between `ethnic_group` and `parent educ` and adjusted R^2 value decreased. Too many covariates from the interaction. Will not be keeping this.

# Add lunch_type covariate and added interaction effect between `parent_marital_status` and `wkly_study_hours`
reading_model2v2 <- lm(transformed_reading ~ gender + parent_educ + lunch_type + ethnic_group + test_prep + parent_marital_status + wkly_study_hours + parent_marital_status*wkly_study_hours, data = score_df)
summary(reading_model2v2)

### lunch_type is a significant term and the interaction also is significant.  However, ethnic_group variables demonstrates to be less significant when lunch_type is added.

# Add interaction between `gender`/ `wkly_study_hours` / `test_prep` and `parent_educ`.
reading_model2v3 <- lm(transformed_reading ~ gender + lunch_type + parent_marital_status + gender*parent_educ + test_prep*parent_educ, data = score_df)
summary(reading_model2v3)

## Some significant interaction effect between `test_prep` and `parent_educ` and adjusted R^2 slightly decreased so will be not be keeping these interactions. Keeping reading_model2v2. 
```

```{r Model 2}
reading_model2 <- lm(transformed_reading ~ gender + parent_educ + lunch_type + ethnic_group + test_prep + parent_marital_status + wkly_study_hours + parent_marital_status*wkly_study_hours, data = score_df)

reading_model2 |> 
  broom::tidy() |> 
  mutate(`p.value` = signif(`p.value`, 3), 
         estimate = round(estimate, 3), 
         `std.error` = round(`std.error`, 3), 
         statistic = round(statistic, 3)) |> 
  drop_na()|>
  knitr::kable(digits = 50)

par(mfrow = c(2,2))
plot(reading_model2)

### removing the influential point if needed
cooksd <- cooks.distance(reading_model2)
cooksd_clean <- cooksd[!is.na(cooksd)]
influential <- as.numeric(names(cooksd_clean)[(cooksd_clean > 0.5)])
# influential_read is 0
```

```{r}
### Interaction between `transport_means`/ `is_first_child` with `test_prep`
reading_model3v1 <- lm(transformed_reading ~ gender + lunch_type + parent_marital_status + test_prep*transport_means + test_prep*is_first_child, data = score_df)
summary(reading_model3v1)

### No significant interaction/ Will not be keeping this

### Interaction between `is_first_child` and `parent_educ` and `parent_marital_status` and `wkly_study_hours`
reading_model3v2 <- lm(transformed_reading ~ gender + lunch_type + is_first_child:parent_educ + parent_marital_status:wkly_study_hours + ethnic_group:test_prep, data = score_df)
summary(reading_model3v2)

### No Significant interaction between wkly_study_hours  and parent_educ but some significance between ethnic_group and test_prep and adjusted R^2 increased slightly. Note that this model added many covariates due to the interaction terms.

## Add 3 way interaction between `ethnic_group`, `parent_educ`, `gender`.
reading_model3v3 <- lm(transformed_reading ~ test_prep + lunch_type + ethnic_group*parent_educ*gender, data = score_df)
summary(reading_model3v3)
### Few significant 3-way interaction effect and the adjusted R^2 decreasing so not choosing these interactions

### will be keeping model3v2

```

```{r Model 3}
reading_model3 <- lm(transformed_reading ~ gender + lunch_type + is_first_child:parent_educ + parent_marital_status:wkly_study_hours + ethnic_group:test_prep, data = score_df)

reading_model3 |> 
  broom::tidy() |> 
  mutate(`p.value` = signif(`p.value`, 3), 
         estimate = round(estimate, 3), 
         `std.error` = round(`std.error`, 3), 
         statistic = round(statistic, 3)) |> 
  knitr::kable(digits = 50)


par(mfrow = c(2,2))
plot(reading_model3)

### removing the influential point if needed
cooksd <- cooks.distance(reading_model3)
influential <- as.numeric(names(cooksd)[(cooksd > 0.5)])
## no influential points
```


```{r}
### added `nr_siblings` variables instead of `wkly_study_hours`
reading_model4v1 <- lm(transformed_reading ~ gender + ethnic_group + parent_educ + test_prep + parent_marital_status + nr_siblings, data = score_df)
summary(reading_model4v1)

### nr_siblilngs is not significant. will not be adding this. 

### added `transport_means`
reading_model4v2 <- lm(transformed_reading ~ gender + ethnic_group + parent_educ + test_prep + parent_marital_status + wkly_study_hours + transport_means, data = score_df)
summary(reading_model4v2)

### transport_means is not significant. will not be adding this. 

### will test reading_model1, reading_model2 and reading_model3.
```

## Splitting dataset for cross validation
```{r CV, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 4, out.width = "95%", dpi = 600}
#specify the cross-validation method
set.seed(1)
ctrl <- trainControl(method = "cv", number = 5)

filtered_df <- score_df |> 
  drop_na(gender, parent_educ, test_prep, lunch_type, parent_marital_status, wkly_study_hours, ethnic_group)

#fit a regression model and use k-fold CV to evaluate performance
model1 <- train(transformed_reading ~ gender + ethnic_group + parent_educ + test_prep + parent_marital_status + wkly_study_hours, data = filtered_df, method = "lm", trControl = ctrl)

model2 <- train(transformed_reading ~ gender + parent_educ + lunch_type + ethnic_group + test_prep + parent_marital_status + wkly_study_hours + parent_marital_status*wkly_study_hours, data = filtered_df, method = "lm", trControl = ctrl)

model3 <- train(transformed_reading ~ gender + lunch_type + is_first_child:parent_educ + parent_marital_status:wkly_study_hours + ethnic_group:test_prep, data = filtered_df, method = "lm", trControl = ctrl)

# get fold subsets
fold_data_model1 <- lapply(model1$control$index, function(index) filtered_df[index,]) |> 
    bind_rows(.id = "Fold") 
fold_data_model2 <- lapply(model2$control$index, function(index) filtered_df[index,]) |> 
    bind_rows(.id = "Fold") 
fold_data_model3 <- lapply(model3$control$index, function(index) filtered_df[index,]) |> 
    bind_rows(.id = "Fold") 

# example plots
plot1 <- ggplot(fold_data_model1, aes(reading_score, col = Fold)) + geom_density(alpha = 0.6) + ggtitle("Model1")
plot2 <- ggplot(fold_data_model2, aes(reading_score, col = Fold)) + geom_density(alpha = 0.6) + ggtitle("Model2")
plot3 <- ggplot(fold_data_model3, aes(reading_score, col = Fold)) + geom_density(alpha = 0.6) + ggtitle("Model3")
ggarrange(plot1, plot2, plot3, ncol = 3, common.legend = TRUE)

```

Below is the 5-fold cross-validation results including fitted value versus observed score plot and performace metrices from each of the three models. 

```{r Pred Reading Score, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 4, out.width = "95%", dpi = 600}
filtered_df |> 
  mutate(reading_model1_pred = (model1[["finalModel"]][["fitted.values"]])^(1/1.44) - 1, 
         reading_model2_pred = (model2[["finalModel"]][["fitted.values"]])^(1/1.44) - 1, 
         reading_model3_pred = (model3[["finalModel"]][["fitted.values"]])^(1/1.44) - 1) |> 
  pivot_longer(
    cols = c(reading_model1_pred, reading_model2_pred, reading_model3_pred), 
    names_to = "model_type", 
    values_to = "res"
  ) |> 
  ggplot(aes(x = res, y = reading_score)) +
  geom_abline(intercept = 0, slope = 1, color = "#f7aa58") +
  geom_point(size = 1, shape = 21) +
  facet_wrap(~model_type, scale = "free") +
  stat_cor(label.y = 5) +
  scale_x_discrete(labels = c("Predicted Reading Score"))

bind_rows(model1$results, model2$results, model3$results) |> 
  mutate(model_id = c(1, 2, 3)) |> 
  relocate(model_id) |> 
  knitr::kable(digits =3, caption = "Performance matrices of the 3 Multiple Linear Regression Models")

```

Based on the results, the model with the lowest RMSE value indicates highest accuracy. Therefore, the final model will be `reading_model2`.
```{r}
final_model = reading_model2
summary(final_model)
```


